{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/minrei/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/minrei/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/minrei/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/minrei/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import string\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# optimization\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from hyperopt import fmin, tpe, hp # to use with gensim models\n",
    "\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kickers on my watchlist XIDE TIT SOQ PNK CPW B...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user: AAP MOVIE. 55% return for the FEA/GEED i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user I'd be afraid to short AMZN - they are lo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MNTA Over 12.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OI  Over 21.37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Sentiment\n",
       "0  Kickers on my watchlist XIDE TIT SOQ PNK CPW B...          1\n",
       "1  user: AAP MOVIE. 55% return for the FEA/GEED i...          1\n",
       "2  user I'd be afraid to short AMZN - they are lo...          1\n",
       "3                                  MNTA Over 12.00            1\n",
       "4                                   OI  Over 21.37            1"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "df = pd.read_csv(\"stock_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "1. Cleaning and normalizing.\n",
    "2. Remove stop words\n",
    "3. Stemming and lemmatization\n",
    "4. Create a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kickers on my watchlist XIDE TIT SOQ PNK CPW B...</td>\n",
       "      <td>1</td>\n",
       "      <td>kicker watchlist xide tit soq pnk cpw bpz aj t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user: AAP MOVIE. 55% return for the FEA/GEED i...</td>\n",
       "      <td>1</td>\n",
       "      <td>user aap movie 55 return feageed indicator 15 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user I'd be afraid to short AMZN - they are lo...</td>\n",
       "      <td>1</td>\n",
       "      <td>user id afraid short amzn looking like nearmon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MNTA Over 12.00</td>\n",
       "      <td>1</td>\n",
       "      <td>mnta 1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OI  Over 21.37</td>\n",
       "      <td>1</td>\n",
       "      <td>oi 2137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Sentiment  \\\n",
       "0  Kickers on my watchlist XIDE TIT SOQ PNK CPW B...          1   \n",
       "1  user: AAP MOVIE. 55% return for the FEA/GEED i...          1   \n",
       "2  user I'd be afraid to short AMZN - they are lo...          1   \n",
       "3                                  MNTA Over 12.00            1   \n",
       "4                                   OI  Over 21.37            1   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  kicker watchlist xide tit soq pnk cpw bpz aj t...  \n",
       "1  user aap movie 55 return feageed indicator 15 ...  \n",
       "2  user id afraid short amzn looking like nearmon...  \n",
       "3                                          mnta 1200  \n",
       "4                                            oi 2137  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleans & normalizes data\n",
    "def preprocess_data(text):\n",
    "    # Remove punctuation and special characters\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "    # Lowercase all the words\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    tokens = [token for token in tokens if token not in stopwords.words(\"english\")]\n",
    "    \n",
    "    # Lemmatize the words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Return the cleaned, normalized, and pre-processed text\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"cleaned_text\"] = df[\"Text\"].apply(preprocess_data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10937"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create vocabulary from processed words\n",
    "def create_vocabulary(text):\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    vocabulary = set(tokens)\n",
    "\n",
    "    return vocabulary\n",
    "\n",
    "# Build vocabulary\n",
    "vocabulary = set()\n",
    "for vocab in df[\"cleaned_text\"].apply(create_vocabulary):\n",
    "    vocabulary = vocabulary.union(vocab)\n",
    "\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing a model architecture\n",
    "\n",
    "We shall begin with a basic bag-of-words model. This involves representing each comment as a feature vector, where each element of the vector is the count of a specific word in the vocabulary. Next, we train a classification model such as logistic regression, on the feature vectors and their corresponding labels to make predictions about the sentiment of new comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4632, 10861)\n",
      "(1159, 10861)\n",
      "(4632,)\n",
      "(1159,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7739430543572045"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a bag-of-words representation of the data\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"cleaned_text\"])\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df[\"Sentiment\"], test_size=0.2)\n",
    "\n",
    "# Fit train data to a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate performance on test data\n",
    "score = model.score(X_test, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better models\n",
    "\n",
    "Let us examine if we can achieve a higher score(accuracy) than 0.77. We consider two other model architectures: TF-IDF and word embeddings.\n",
    "\n",
    "The former improves on the bag-of-words model by assigning higher weights to more informative and unique words, and lower weights to common words. \n",
    "\n",
    "The latter represents words in a low-dimensional space. These vectors capture the semantic and syntactic *relationships* between words.\n",
    "\n",
    "Since TF-IDF is a statistical method based on the frequency of words, it is simpler and interpretable compared to the more complex black-box model of word embeddings.\n",
    "\n",
    "However, TF-IDF ignores the word order and gramamtical structure while word embeddings capture the context and meaning of words in a sentence.\n",
    "\n",
    "Therefore, we shall use *word embeddings* to perform our binary sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:55<00:00,  1.16s/trial, best loss: -0.7144089732528042]\n"
     ]
    }
   ],
   "source": [
    "# Create a list of sentences\n",
    "sentences = df[\"cleaned_text\"].str.split()\n",
    "\n",
    "# Define hyperparameter space for the Word2Vec model\n",
    "space = {\n",
    "    # \"min_count\": hp.quniform(\"min_count\", 1, 3, 1),\n",
    "    \"window\": hp.quniform(\"window\", 3, 7, 2),\n",
    "    \"sg\": hp.choice(\"sg\", [0, 1]),\n",
    "    \"workers\": hp.quniform(\"workers\", 4, 16, 2)\n",
    "}\n",
    "# Define the objective function for the Word2Vec model\n",
    "def objective(params):\n",
    "    # Create an instance of the Word2Vec model with the given hyperparameters\n",
    "    w2v = Word2Vec(sentences=sentences, min_count=1, window=params[\"window\"], sg=params[\"sg\"], workers=params[\"workers\"])\n",
    "\n",
    "    w2v.train(sentences, epochs=w2v.epochs, total_examples=w2v.corpus_count)\n",
    "\n",
    "    X = []\n",
    "    for sentence in sentences:\n",
    "        vector = w2v.wv[sentence].mean(axis=0)\n",
    "        X.append(vector)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, df[\"Sentiment\"], test_size=0.2)\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate\n",
    "    score = model.score(X_test, y_test)\n",
    "    return -score\n",
    "\n",
    "\n",
    "# Use the fmin function to find the optimal values for the hyperparameters\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "In spite of hyperoptimization, the best we achieve is an accuracy of 0.71 with our word embedding model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sg': 1, 'window': 6.0, 'workers': 14.0}\n"
     ]
    }
   ],
   "source": [
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nvda_fintech')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17a9839d355cc62e9348dd3d7e2e30ac9a631a5665193c53246c17745514bd34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
