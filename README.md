# nlp_fintech

1. Preprocessing: cleaning and normalizing, possibly stemming or lemmatization.

2. Choose a model architecture: bag-of-words, word embedding

3. Training: employ a train-test split and fit our data to the train dataset.

4. Parameter optimization: typically involves a gradient-based optimization algorithm such as stochastic gradient descent (SGD) or Adam.

4. Evaluation: use trained model to make predictions on the test set. We calculate various metrics such as **accuracy**, precision, and recall to assess the model's performance.

5. Fine-tuning: further improve performance by fine-tuning hyperparameters, such as the learning rate or the size of the hidden layers. Techniques include grid search or random search to try out different combinations of hyperparameters and see which ones yield the best performance.

6. Make predictions: feeding the preprocessed data into the trained model to generate predictions.

